{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "import math\n",
    "# 2018/3/3 Kitamura Add Start\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# 2018/3/3 Kitamura Add End\n",
    "\n",
    "#スタート時間を保持\n",
    "starttime = time.time()\n",
    "\n",
    "# 学習の設定\n",
    "l_of_s         = 10\n",
    "n_next         = 3\n",
    "in_out_neurons = 1\n",
    "hidden_neurons = 200\n",
    "batch_size = 10\n",
    "\n",
    "# 学習用データを抽出する関数\n",
    "def _load_data(data, n_prev=10, n_next=1, flag=True):\n",
    "    docX, docY = [], []\n",
    "    for i in range(n_prev, len(data)-n_next):\n",
    "        #起点の箇所からn_prevだけ戻った日数分をデータとする\n",
    "        docX.append(data.iloc[i-n_prev:i].as_matrix())\n",
    "        #起点の翌日からn_next日数分進んだデータのmax or minをyデータとする\n",
    "        if flag == True:\n",
    "            docY.append(max(data.iloc[i+1:i+n_next].as_matrix()))\n",
    "        else:\n",
    "            docY.append(min(data.iloc[i+1:i+n_next].as_matrix()))\n",
    "\n",
    "    alsX = numpy.array(docX)\n",
    "    alsY = numpy.array(docY)\n",
    "    alsX\n",
    "    alsY\n",
    "    return alsX, alsY\n",
    "\n",
    "def lstm_model(activation=\"relu\", Optimizer=RMSprop, out_dim=1, hidden_dim=256, learning_rate=0.001, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_dim, \n",
    "          batch_input_shape=(None, l_of_s, out_dim),\n",
    "          return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(out_dim))\n",
    "    model.add(Activation(activation))\n",
    "    optimizer = Optimizer(lr=learning_rate)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def min_max(x, axis=None):\n",
    "    min = x.min(axis=axis, keepdims=True)\n",
    "    max = x.max(axis=axis, keepdims=True)\n",
    "    result = (x-min)/(max-min)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('usdjpy_d.csv', <http.client.HTTPMessage at 0x23017e68208>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "#ダウンロードする通貨ペア\n",
    "#https://stooq.com/q/d/?s=usdjpy\n",
    "#ついでにsekjpy、nokjpy、mxnjpy、sgdjpyとかもありますが、\n",
    "#2000/01/01からデータがない（あるけど、しばらくopen/high/low/closeがすべて一緒)ので\n",
    "#ここには７通貨ペアしか記載しておりません。日付とか調整して通貨ペアを増やしてみても\n",
    "#いいと思います\n",
    "currency_pair = 'usdjpy'\n",
    "#currency_pair = 'eurjpy'\n",
    "#currency_pair = 'gbpjpy'\n",
    "#currency_pair = 'audjpy'\n",
    "#currency_pair = 'cadjpy'\n",
    "#currency_pair = 'chfjpy'\n",
    "#currency_pair = 'nzdjpy'\n",
    "#currency_pair = 'sekjpy'\n",
    "#currency_pair = 'nokjpy'\n",
    "\n",
    "#スタート日付\n",
    "start_day     = \"20150101\"\n",
    "#終了日を今日に指定\n",
    "url           = \"https://stooq.com/q/d/l/?s=\" + currency_pair + \\\n",
    "                \"&d1=\" + start_day + \"&d2=\" + dt.today().strftime(\"%Y%m%d\") + \"&i=d\"\n",
    "file_name     = currency_pair + '_d.csv'\n",
    "#取得して、ファイルに保存(よくよく考えると保存しなくてもいいな)\n",
    "urllib.request.urlretrieve(url, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Start: 259\n",
      "Train End  : 517\n",
      "Test  Start: 517\n",
      "Test  End  : 776\n",
      "(U+D)/(U+D+E): 35%\n",
      "(U+D)/(U+D+E): 37%\n",
      "(U+D)/(U+D+E): 40%\n",
      "(U+D)/(U+D+E): 43%\n",
      "(U+D)/(U+D+E): 45%\n",
      "(U+D)/(U+D+E): 49%\n",
      "(U+D)/(U+D+E): 51%\n",
      "(U+D)/(U+D+E): 54%\n",
      "(U+D)/(U+D+E): 56%\n",
      "(U+D)/(U+D+E): 60%\n",
      "(U+D)/(U+D+E): 63%\n",
      "(U+D)/(U+D+E): 64%\n",
      "(U+D)/(U+D+E): 66%\n",
      "p            : 0.4600000000000002\n",
      "UP   COUNT   : 156\n",
      "DOWN COUNT   : 189\n",
      "EVNE COUNT   : 172\n",
      "(U+D)/(U+D+E): 66%\n",
      "max: 125.85\n",
      "min: 99.101\n",
      "ave: 112.4755\n",
      "dif: 13.374499999999998\n"
     ]
    }
   ],
   "source": [
    "# FXデータの読み込み\n",
    "data = None\n",
    "data = pandas.read_csv(file_name)\n",
    "data.columns = ['date', 'open', 'high', 'low', 'close']\n",
    "data['date'] = pandas.to_datetime(data['date'], format='%Y-%m-%d')\n",
    "\n",
    "#train開始終了、test開始終了日を設定\n",
    "#データは2001/01/01からとっていますが、さかのぼって調査する関係上\n",
    "#20001/01/01からにしています。また買い目対象の調査としてtest_end_dayよりも\n",
    "#後の日も参照するためtest_end_dayの取得するデータにはしないでください。\n",
    "#trainの終了日とtestの開始日が連続していますが、連続させなくても動作するか\n",
    "#検証していません。注意してください\n",
    "train_start_day   = dt.strptime('2016-01-01', '%Y-%m-%d')\n",
    "train_end_day     = dt.strptime('2016-12-31', '%Y-%m-%d')\n",
    "test_start_day    = dt.strptime('2017-01-01', '%Y-%m-%d')\n",
    "test_end_day      = dt.strptime('2017-12-31', '%Y-%m-%d')\n",
    "train_start_count = -1\n",
    "train_end_count   = -1\n",
    "test_start_count  = -1\n",
    "test_end_count    = -1\n",
    "\n",
    "#train/testの開始終了日の配列の場所を調査\n",
    "for i in range(len(data['date'])):\n",
    "    if train_start_count == -1 and data['date'][i] >= train_start_day:\n",
    "        train_start_count = i\n",
    "    if train_end_count == -1 and data['date'][i] >= train_end_day:\n",
    "        train_end_count = i\n",
    "    if test_start_count == -1 and data['date'][i] >= test_start_day:\n",
    "        test_start_count = i\n",
    "    if test_end_count == -1 and data['date'][i] >= test_end_day:\n",
    "        test_end_count = i\n",
    "        break\n",
    "\n",
    "#前にl_of_s日分、後ろにn_next分日数が必要なので\n",
    "#チェック。足りない場合は中止。これを考慮に入れて\n",
    "#train/testの開始終了日を設定してください\n",
    "if train_start_count - l_of_s < 0 or \\\n",
    "    test_end_count + n_next > len(data['date']):\n",
    "    \n",
    "    print(\"data range over\")\n",
    "    sys.exit()\n",
    "    \n",
    "print('Train Start: ' + str(train_start_count))\n",
    "print('Train End  : ' + str(train_end_count))\n",
    "print('Test  Start: ' + str(test_start_count))\n",
    "print('Test  End  : ' + str(test_end_count))\n",
    "\n",
    "up_down           = []\n",
    "up_count          = 0\n",
    "down_count        = 0\n",
    "even_count        = 0\n",
    "#up/downの割合\n",
    "check_treshhold   = 0.666\n",
    "loop_flag         = True\n",
    "\n",
    "#\n",
    "check_add_percent = 0.0020\n",
    "check_percent     = 0.02\n",
    "if min(data.loc[:, 'low']) > 150:\n",
    "    check_add_percent = 0.20\n",
    "    check_percent     = 2.0\n",
    "elif min(data.loc[:, 'low']) > 30:\n",
    "    check_add_percent = 0.020\n",
    "    check_percent     = 0.2\n",
    "    \n",
    "#close_open_diff = numpy.array([])\n",
    "#for i in range(train_start_count, test_end_count):\n",
    "#    close_open_diff = numpy.append(close_open_diff, numpy.array(data.loc[i-1, 'close'] - data.loc[i, 'open']))\n",
    "#print('Close-Open Diff:' + str(close_open_diff.mean()) + ' ' + str(close_open_diff.std()))\n",
    "#up_c_o_diff   = close_open_diff.mean() + close_open_diff.std() * 2\n",
    "#down_c_o_diff = close_open_diff.mean() - close_open_diff.std() * 2\n",
    "#c_o_d_remove_count = 0\n",
    "#for i in range(train_start_count, test_end_count):\n",
    "#    if data.loc[i-1, 'close'] - data.loc[i, 'open'] >= up_c_o_diff or \\\n",
    "#        data.loc[i-1, 'close'] - data.loc[i, 'open'] <= down_c_o_diff:\n",
    "#        c_o_d_remove_count += 1\n",
    "#print(\"c_o_d_remove_count:\" + str(c_o_d_remove_count))\n",
    "\n",
    "while loop_flag:\n",
    "    up_count = 0\n",
    "    down_count = 0\n",
    "    even_count = 0\n",
    "    check_percent += check_add_percent\n",
    "    for i in range(train_start_count, test_end_count):\n",
    "        #起点の日の翌日のopenの値\n",
    "        open_value = data.loc[i+1, 'open']\n",
    "        #起点の日から翌日からn_next日数分のhighの最大値\n",
    "        max_value = max(data.loc[i+1:i+n_next, 'high'])\n",
    "        #起点の日から翌日からn_next日数分のlowの最小値\n",
    "        min_value = min(data.loc[i+1:i+n_next, 'low'])\n",
    "        #ここは起点の日の翌日のopenの値と起点の日の翌日から\n",
    "        #n_next日数分のhighの最高値かlowの最小値が規定の値上に\n",
    "        #差が広がったカウントを調べる。これで上がった・下がったを\n",
    "        #回数を調べる\n",
    "        \n",
    "        if abs(max_value - open_value) >= check_percent and \\\n",
    "            abs(open_value - min_value) < check_percent:\n",
    "            up_count += 1\n",
    "        elif abs(open_value - min_value) >= check_percent and \\\n",
    "            abs(max_value - open_value) < check_percent:\n",
    "            down_count += 1  \n",
    "        else:\n",
    "            even_count += 1\n",
    "    \n",
    "    #(上がった日+下がった日)/全体の日数でcheck_treshholdを超えたか調べる\n",
    "    #超えていればその値をベースにする\n",
    "    print('(U+D)/(U+D+E): ' + str(math.floor((up_count + down_count) / \\\n",
    "                                             (up_count + down_count +even_count) * 100)) + '%')    \n",
    "    if (up_count + down_count) / (up_count + down_count +even_count) > check_treshhold:\n",
    "        break\n",
    "\n",
    "#上がった・下がったの判定するための数字（もっと言うと円がどれだけ動いたか）\n",
    "print('p            : ' + str(check_percent))\n",
    "print(\"UP   COUNT   : \" + str(up_count))\n",
    "print(\"DOWN COUNT   : \" + str(down_count))\n",
    "print(\"EVNE COUNT   : \" + str(even_count))\n",
    "print('(U+D)/(U+D+E): ' + str(math.floor((up_count + down_count) / \\\n",
    "                                         (up_count + down_count +even_count) * 100)) + '%')    \n",
    "\n",
    "#sys.exit()\n",
    "#上がった・下がったの判定用\n",
    "#ここではtestの範囲のみ取得\n",
    "for i in range(test_start_count, test_end_count):\n",
    "    open_value = data.loc[i+1, 'open']\n",
    "    close_value = data.loc[i, 'open']\n",
    "    #起点の日から翌日からn_next日数分のhighの最大値\n",
    "    max_value = max(data.loc[i+1:i+n_next, 'high'])\n",
    "    #起点の日から翌日からn_next日数分のlowの最小値\n",
    "    min_value = min(data.loc[i+1:i+n_next, 'low'])\n",
    "    #ここは起点の日の翌日のopenの値と起点の日の翌日から\n",
    "    #n_next日数分のhighの最高値かlowの最小値が規定の値上に\n",
    "    #差が広がったカウントを調べる。これで上がった・下がったを\n",
    "    #回数を調べる\n",
    "    if abs(max_value - open_value) >= check_percent and \\\n",
    "        abs(open_value - min_value) < check_percent:\n",
    "        up_down.append(1)\n",
    "    elif abs(open_value - min_value) >= check_percent and \\\n",
    "        abs(max_value - open_value) < check_percent:\n",
    "        up_down.append(-1)\n",
    "    else:\n",
    "        up_down.append(0)\n",
    "\n",
    "max_value = max(data['high'])\n",
    "min_value = min(data['low'])\n",
    "average_value = (max_value+min_value)/2\n",
    "diff_value = max_value - average_value\n",
    "print ('max: ' + str(max_value))\n",
    "print ('min: ' + str(min_value))\n",
    "print ('ave: ' + str(average_value))\n",
    "print ('dif: ' + str(diff_value))\n",
    "\n",
    "#for i in range(len(data['high'].index)):\n",
    "#data.loc[i, 'high'] = (data.loc[i, 'high'] - average_value) / diff_value\n",
    "#data.loc[i, 'low']  = (data.loc[i, 'low'] - average_value) / diff_value\n",
    "#data.loc[i, 'open'] = (data.loc[i, 'open'] - average_value) / diff_value\n",
    "#data.loc[i, 'close'] = (data.loc[i, 'close'] - average_value) / diff_value\n",
    "\n",
    "# 2018/3/3 Kitamura add Start\n",
    "# 正規化\n",
    "#scaler  = MinMaxScaler( feature_range=(0, 1) )    \n",
    "#data.loc[i, 'high'] = scaler.fit_transform(data.loc[i, 'high'])\n",
    "#data.loc[i, 'low'] = scaler.fit_transform(data.loc[i, 'low'])\n",
    "#data.loc[i, 'open'] = scaler.fit_transform(data.loc[i, 'open'])\n",
    "#data.loc[i, 'close'] = scaler.fit_transform(data.loc[i, 'close'])\n",
    "# 2018/3/3 Kitamura add End\n",
    "\n",
    "data = data.sort_values(by='date')\n",
    "data = data.reset_index(drop=True)\n",
    "data = data.loc[:, ['date', 'open','high', 'low', 'close']]\n",
    "\n",
    "#グラフ化\n",
    "#plt.plot(data['date'], data['high'])\n",
    "#plt.plot(data['date'], data['low'])\n",
    "#plt.show()\n",
    "\n",
    "#sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=256, nb_epoch=10, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 1s - loss: 5899.0085 - acc: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaoru.kitamura\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:497: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: acc,loss\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=256, nb_epoch=10, activation=sigmoid, total=   3.5s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=256, nb_epoch=10, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 1s - loss: 6283.6112 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=256, nb_epoch=10, activation=sigmoid, total=   3.0s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=256, nb_epoch=10, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 2s - loss: 5809.7310 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=256, nb_epoch=10, activation=sigmoid, total=   3.5s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=256, nb_epoch=100, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 2s - loss: 5902.5919 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=256, nb_epoch=100, activation=sigmoid, total=   4.0s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=256, nb_epoch=100, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 2s - loss: 6286.2103 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=256, nb_epoch=100, activation=sigmoid, total=   4.0s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=256, nb_epoch=100, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 2s - loss: 5809.9236 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=256, nb_epoch=100, activation=sigmoid, total=   4.3s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Adamax'>, out_dim=1, dropout_rate=0.5, learning_rate=0.001, hidden_dim=128, nb_epoch=10, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 1s - loss: 5953.6318 - acc: 0.0000e+00\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Adamax'>, out_dim=1, dropout_rate=0.5, learning_rate=0.001, hidden_dim=128, nb_epoch=10, activation=sigmoid, total=   3.1s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Adamax'>, out_dim=1, dropout_rate=0.5, learning_rate=0.001, hidden_dim=128, nb_epoch=10, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 1s - loss: 6336.2663 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Adamax'>, out_dim=1, dropout_rate=0.5, learning_rate=0.001, hidden_dim=128, nb_epoch=10, activation=sigmoid, total=   3.7s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Adamax'>, out_dim=1, dropout_rate=0.5, learning_rate=0.001, hidden_dim=128, nb_epoch=10, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 2s - loss: 5857.7947 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Adamax'>, out_dim=1, dropout_rate=0.5, learning_rate=0.001, hidden_dim=128, nb_epoch=10, activation=sigmoid, total=   3.4s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Adamax'>, out_dim=1, dropout_rate=0.2, learning_rate=0.001, hidden_dim=128, nb_epoch=10, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 1s - loss: 5952.8080 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Adamax'>, out_dim=1, dropout_rate=0.2, learning_rate=0.001, hidden_dim=128, nb_epoch=10, activation=sigmoid, total=   3.3s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Adamax'>, out_dim=1, dropout_rate=0.2, learning_rate=0.001, hidden_dim=128, nb_epoch=10, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 2s - loss: 6338.4805 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Adamax'>, out_dim=1, dropout_rate=0.2, learning_rate=0.001, hidden_dim=128, nb_epoch=10, activation=sigmoid, total=   3.7s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Adamax'>, out_dim=1, dropout_rate=0.2, learning_rate=0.001, hidden_dim=128, nb_epoch=10, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 2s - loss: 5861.2922 - acc: 0.0116\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Adamax'>, out_dim=1, dropout_rate=0.2, learning_rate=0.001, hidden_dim=128, nb_epoch=10, activation=sigmoid, total=   3.8s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=256, nb_epoch=10, activation=softmax \n",
      "Epoch 1/1\n",
      " - 2s - loss: 5888.9186 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=256, nb_epoch=10, activation=softmax, total=   4.5s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=256, nb_epoch=10, activation=softmax \n",
      "Epoch 1/1\n",
      " - 3s - loss: 6272.0639 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=256, nb_epoch=10, activation=softmax, total=   4.9s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=256, nb_epoch=10, activation=softmax \n",
      "Epoch 1/1\n",
      " - 3s - loss: 5796.4593 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=256, nb_epoch=10, activation=softmax, total=   4.9s\n",
      "[CV] batch_size=10, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.1, learning_rate=0.5, hidden_dim=128, nb_epoch=10, activation=linear \n",
      "Epoch 1/1\n",
      " - 2s - loss: 2113.6882 - acc: 0.0058\n",
      "[CV]  batch_size=10, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.1, learning_rate=0.5, hidden_dim=128, nb_epoch=10, activation=linear, total=   4.4s\n",
      "[CV] batch_size=10, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.1, learning_rate=0.5, hidden_dim=128, nb_epoch=10, activation=linear \n",
      "Epoch 1/1\n",
      " - 2s - loss: 2315.6721 - acc: 0.0116\n",
      "[CV]  batch_size=10, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.1, learning_rate=0.5, hidden_dim=128, nb_epoch=10, activation=linear, total=   5.1s\n",
      "[CV] batch_size=10, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.1, learning_rate=0.5, hidden_dim=128, nb_epoch=10, activation=linear \n",
      "Epoch 1/1\n",
      " - 2s - loss: 2083.4137 - acc: 0.0058\n",
      "[CV]  batch_size=10, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.1, learning_rate=0.5, hidden_dim=128, nb_epoch=10, activation=linear, total=   5.1s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=256, nb_epoch=5, activation=hard_sigmoid \n",
      "Epoch 1/1\n",
      " - 2s - loss: 5900.8333 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=256, nb_epoch=5, activation=hard_sigmoid, total=   5.2s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=256, nb_epoch=5, activation=hard_sigmoid \n",
      "Epoch 1/1\n",
      " - 3s - loss: 6281.3348 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=256, nb_epoch=5, activation=hard_sigmoid, total=   5.7s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=256, nb_epoch=5, activation=hard_sigmoid \n",
      "Epoch 1/1\n",
      " - 3s - loss: 5807.6239 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=256, nb_epoch=5, activation=hard_sigmoid, total=   6.2s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Adamax'>, out_dim=1, dropout_rate=0.1, learning_rate=0.5, hidden_dim=128, nb_epoch=100, activation=linear \n",
      "Epoch 1/1\n",
      " - 3s - loss: 1802.1070 - acc: 0.0116\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Adamax'>, out_dim=1, dropout_rate=0.1, learning_rate=0.5, hidden_dim=128, nb_epoch=100, activation=linear, total=   5.6s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Adamax'>, out_dim=1, dropout_rate=0.1, learning_rate=0.5, hidden_dim=128, nb_epoch=100, activation=linear \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 3s - loss: 2031.0759 - acc: 0.0000e+00\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Adamax'>, out_dim=1, dropout_rate=0.1, learning_rate=0.5, hidden_dim=128, nb_epoch=100, activation=linear, total=   5.3s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Adamax'>, out_dim=1, dropout_rate=0.1, learning_rate=0.5, hidden_dim=128, nb_epoch=100, activation=linear \n",
      "Epoch 1/1\n",
      " - 3s - loss: 1621.9858 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Adamax'>, out_dim=1, dropout_rate=0.1, learning_rate=0.5, hidden_dim=128, nb_epoch=100, activation=linear, total=   6.3s\n",
      "[CV] batch_size=10, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=512, nb_epoch=100, activation=linear \n",
      "Epoch 1/1\n",
      " - 5s - loss: 4028.3538 - acc: 0.0058\n",
      "[CV]  batch_size=10, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=512, nb_epoch=100, activation=linear, total=   8.0s\n",
      "[CV] batch_size=10, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=512, nb_epoch=100, activation=linear \n",
      "Epoch 1/1\n",
      " - 5s - loss: 2469.4062 - acc: 0.0058\n",
      "[CV]  batch_size=10, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=512, nb_epoch=100, activation=linear, total=   8.1s\n",
      "[CV] batch_size=10, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=512, nb_epoch=100, activation=linear \n",
      "Epoch 1/1\n",
      " - 5s - loss: 2480.9588 - acc: 0.0000e+00\n",
      "[CV]  batch_size=10, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=512, nb_epoch=100, activation=linear, total=   8.6s\n",
      "[CV] batch_size=10, Optimizer=<class 'keras.optimizers.SGD'>, out_dim=1, dropout_rate=0.2, learning_rate=0.01, hidden_dim=256, nb_epoch=100, activation=tanh \n",
      "Epoch 1/1\n",
      " - 4s - loss: 5897.4893 - acc: 0.0058\n",
      "[CV]  batch_size=10, Optimizer=<class 'keras.optimizers.SGD'>, out_dim=1, dropout_rate=0.2, learning_rate=0.01, hidden_dim=256, nb_epoch=100, activation=tanh, total=   7.1s\n",
      "[CV] batch_size=10, Optimizer=<class 'keras.optimizers.SGD'>, out_dim=1, dropout_rate=0.2, learning_rate=0.01, hidden_dim=256, nb_epoch=100, activation=tanh \n",
      "Epoch 1/1\n",
      " - 4s - loss: 6282.1700 - acc: 0.0058\n",
      "[CV]  batch_size=10, Optimizer=<class 'keras.optimizers.SGD'>, out_dim=1, dropout_rate=0.2, learning_rate=0.01, hidden_dim=256, nb_epoch=100, activation=tanh, total=   7.4s\n",
      "[CV] batch_size=10, Optimizer=<class 'keras.optimizers.SGD'>, out_dim=1, dropout_rate=0.2, learning_rate=0.01, hidden_dim=256, nb_epoch=100, activation=tanh \n",
      "Epoch 1/1\n",
      " - 4s - loss: 5805.6053 - acc: 0.0058\n",
      "[CV]  batch_size=10, Optimizer=<class 'keras.optimizers.SGD'>, out_dim=1, dropout_rate=0.2, learning_rate=0.01, hidden_dim=256, nb_epoch=100, activation=tanh, total=   6.8s\n",
      "[CV] batch_size=10, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.1, learning_rate=0.001, hidden_dim=128, nb_epoch=10, activation=relu \n",
      "Epoch 1/1\n",
      " - 6s - loss: 6022.7303 - acc: 0.0058\n",
      "[CV]  batch_size=10, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.1, learning_rate=0.001, hidden_dim=128, nb_epoch=10, activation=relu, total=   9.6s\n",
      "[CV] batch_size=10, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.1, learning_rate=0.001, hidden_dim=128, nb_epoch=10, activation=relu \n",
      "Epoch 1/1\n",
      " - 5s - loss: 6402.8948 - acc: 0.0058\n",
      "[CV]  batch_size=10, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.1, learning_rate=0.001, hidden_dim=128, nb_epoch=10, activation=relu, total=   9.1s\n",
      "[CV] batch_size=10, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.1, learning_rate=0.001, hidden_dim=128, nb_epoch=10, activation=relu \n",
      "Epoch 1/1\n",
      " - 5s - loss: 5928.8553 - acc: 0.0058\n",
      "[CV]  batch_size=10, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.1, learning_rate=0.001, hidden_dim=128, nb_epoch=10, activation=relu, total=  11.2s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=128, nb_epoch=100, activation=tanh \n",
      "Epoch 1/1\n",
      " - 4s - loss: 5895.8421 - acc: 0.0116\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=128, nb_epoch=100, activation=tanh, total=   7.8s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=128, nb_epoch=100, activation=tanh \n",
      "Epoch 1/1\n",
      " - 4s - loss: 6278.3936 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=128, nb_epoch=100, activation=tanh, total=   9.4s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=128, nb_epoch=100, activation=tanh \n",
      "Epoch 1/1\n",
      " - 4s - loss: 5803.0751 - acc: 0.0116\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=128, nb_epoch=100, activation=tanh, total=   7.2s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=128, nb_epoch=100, activation=softmax \n",
      "Epoch 1/1\n",
      " - 5s - loss: 5888.9186 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=128, nb_epoch=100, activation=softmax, total=   7.5s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=128, nb_epoch=100, activation=softmax \n",
      "Epoch 1/1\n",
      " - 4s - loss: 6272.0639 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=128, nb_epoch=100, activation=softmax, total=   7.1s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=128, nb_epoch=100, activation=softmax \n",
      "Epoch 1/1\n",
      " - 5s - loss: 5796.4593 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=128, nb_epoch=100, activation=softmax, total=   7.6s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.1, learning_rate=5.0, hidden_dim=512, nb_epoch=100, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 5s - loss: 5901.5203 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.1, learning_rate=5.0, hidden_dim=512, nb_epoch=100, activation=sigmoid, total=   8.7s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.1, learning_rate=5.0, hidden_dim=512, nb_epoch=100, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 5s - loss: 6283.4255 - acc: 0.0000e+00\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.1, learning_rate=5.0, hidden_dim=512, nb_epoch=100, activation=sigmoid, total=   9.6s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.1, learning_rate=5.0, hidden_dim=512, nb_epoch=100, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 5s - loss: 5809.0717 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.1, learning_rate=5.0, hidden_dim=512, nb_epoch=100, activation=sigmoid, total=   8.8s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.2, learning_rate=0.01, hidden_dim=128, nb_epoch=5, activation=linear \n",
      "Epoch 1/1\n",
      " - 4s - loss: 5221.8418 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.2, learning_rate=0.01, hidden_dim=128, nb_epoch=5, activation=linear, total=   7.3s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.2, learning_rate=0.01, hidden_dim=128, nb_epoch=5, activation=linear \n",
      "Epoch 1/1\n",
      " - 4s - loss: 5580.4850 - acc: 0.0000e+00\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.2, learning_rate=0.01, hidden_dim=128, nb_epoch=5, activation=linear, total=   6.9s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.2, learning_rate=0.01, hidden_dim=128, nb_epoch=5, activation=linear \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 4s - loss: 5203.5163 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.2, learning_rate=0.01, hidden_dim=128, nb_epoch=5, activation=linear, total=   7.3s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Adam'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=512, nb_epoch=10, activation=linear \n",
      "Epoch 1/1\n",
      " - 8s - loss: 2379.3569 - acc: 0.0116\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Adam'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=512, nb_epoch=10, activation=linear, total=  12.3s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Adam'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=512, nb_epoch=10, activation=linear \n",
      "Epoch 1/1\n",
      " - 8s - loss: 2504.2075 - acc: 0.0000e+00\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Adam'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=512, nb_epoch=10, activation=linear, total=  12.4s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Adam'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=512, nb_epoch=10, activation=linear \n",
      "Epoch 1/1\n",
      " - 9s - loss: 2397.3258 - acc: 0.0000e+00\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Adam'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=512, nb_epoch=10, activation=linear, total=  12.9s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=256, nb_epoch=100, activation=softmax \n",
      "Epoch 1/1\n",
      " - 5s - loss: 5888.9185 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=256, nb_epoch=100, activation=softmax, total=   9.2s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=256, nb_epoch=100, activation=softmax \n",
      "Epoch 1/1\n",
      " - 5s - loss: 6272.0639 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=256, nb_epoch=100, activation=softmax, total=   8.8s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=256, nb_epoch=100, activation=softmax \n",
      "Epoch 1/1\n",
      " - 5s - loss: 5796.4592 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=256, nb_epoch=100, activation=softmax, total=   8.3s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.1, learning_rate=5.0, hidden_dim=128, nb_epoch=10, activation=linear \n",
      "Epoch 1/1\n",
      " - 6s - loss: 164921.8947 - acc: 0.0000e+00\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.1, learning_rate=5.0, hidden_dim=128, nb_epoch=10, activation=linear, total=   9.1s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.1, learning_rate=5.0, hidden_dim=128, nb_epoch=10, activation=linear \n",
      "Epoch 1/1\n",
      " - 6s - loss: 44275.6578 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.1, learning_rate=5.0, hidden_dim=128, nb_epoch=10, activation=linear, total=   9.3s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.1, learning_rate=5.0, hidden_dim=128, nb_epoch=10, activation=linear \n",
      "Epoch 1/1\n",
      " - 6s - loss: 33463.7882 - acc: 0.0000e+00\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.1, learning_rate=5.0, hidden_dim=128, nb_epoch=10, activation=linear, total=  10.0s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.SGD'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=256, nb_epoch=100, activation=hard_sigmoid \n",
      "Epoch 1/1\n",
      " - 5s - loss: 5901.3948 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.SGD'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=256, nb_epoch=100, activation=hard_sigmoid, total=   9.2s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.SGD'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=256, nb_epoch=100, activation=hard_sigmoid \n",
      "Epoch 1/1\n",
      " - 5s - loss: 6284.6495 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.SGD'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=256, nb_epoch=100, activation=hard_sigmoid, total=   8.6s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.SGD'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=256, nb_epoch=100, activation=hard_sigmoid \n",
      "Epoch 1/1\n",
      " - 5s - loss: 5807.5021 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.SGD'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=256, nb_epoch=100, activation=hard_sigmoid, total=   9.0s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Adam'>, out_dim=1, dropout_rate=0.2, learning_rate=0.5, hidden_dim=128, nb_epoch=5, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 5s - loss: 5902.5890 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Adam'>, out_dim=1, dropout_rate=0.2, learning_rate=0.5, hidden_dim=128, nb_epoch=5, activation=sigmoid, total=   8.8s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Adam'>, out_dim=1, dropout_rate=0.2, learning_rate=0.5, hidden_dim=128, nb_epoch=5, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 6s - loss: 6283.0155 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Adam'>, out_dim=1, dropout_rate=0.2, learning_rate=0.5, hidden_dim=128, nb_epoch=5, activation=sigmoid, total=  10.3s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Adam'>, out_dim=1, dropout_rate=0.2, learning_rate=0.5, hidden_dim=128, nb_epoch=5, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 5s - loss: 5807.6510 - acc: 0.0116\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Adam'>, out_dim=1, dropout_rate=0.2, learning_rate=0.5, hidden_dim=128, nb_epoch=5, activation=sigmoid, total=   9.0s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=512, nb_epoch=5, activation=tanh \n",
      "Epoch 1/1\n",
      " - 10s - loss: 5892.7508 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=512, nb_epoch=5, activation=tanh, total=  14.7s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=512, nb_epoch=5, activation=tanh \n",
      "Epoch 1/1\n",
      " - 10s - loss: 6274.8620 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=512, nb_epoch=5, activation=tanh, total=  14.6s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=512, nb_epoch=5, activation=tanh \n",
      "Epoch 1/1\n",
      " - 10s - loss: 5799.8313 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Nadam'>, out_dim=1, dropout_rate=0.5, learning_rate=0.5, hidden_dim=512, nb_epoch=5, activation=tanh, total=  14.9s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=512, nb_epoch=100, activation=softmax \n",
      "Epoch 1/1\n",
      " - 9s - loss: 5888.9186 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=512, nb_epoch=100, activation=softmax, total=  14.3s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=512, nb_epoch=100, activation=softmax \n",
      "Epoch 1/1\n",
      " - 10s - loss: 6272.0640 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=512, nb_epoch=100, activation=softmax, total=  14.7s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=512, nb_epoch=100, activation=softmax \n",
      "Epoch 1/1\n",
      " - 10s - loss: 5796.4593 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=512, nb_epoch=100, activation=softmax, total=  14.8s\n",
      "[CV] batch_size=10, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=512, nb_epoch=10, activation=sigmoid \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 8s - loss: 5893.7915 - acc: 0.0058\n",
      "[CV]  batch_size=10, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=512, nb_epoch=10, activation=sigmoid, total=  13.0s\n",
      "[CV] batch_size=10, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=512, nb_epoch=10, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 13s - loss: 6276.2606 - acc: 0.0058\n",
      "[CV]  batch_size=10, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=512, nb_epoch=10, activation=sigmoid, total=  18.8s\n",
      "[CV] batch_size=10, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=512, nb_epoch=10, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 9s - loss: 5801.4291 - acc: 0.0058\n",
      "[CV]  batch_size=10, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=512, nb_epoch=10, activation=sigmoid, total=  14.1s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=256, nb_epoch=100, activation=tanh \n",
      "Epoch 1/1\n",
      " - 10s - loss: 5894.8361 - acc: 0.0000e+00\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=256, nb_epoch=100, activation=tanh, total=  15.2s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=256, nb_epoch=100, activation=tanh \n",
      "Epoch 1/1\n",
      " - 10s - loss: 6276.7923 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=256, nb_epoch=100, activation=tanh, total=  15.7s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=256, nb_epoch=100, activation=tanh \n",
      "Epoch 1/1\n",
      " - 10s - loss: 5800.6705 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=256, nb_epoch=100, activation=tanh, total=  16.0s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=256, nb_epoch=5, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 8s - loss: 5955.9135 - acc: 0.0116\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=256, nb_epoch=5, activation=sigmoid, total=  14.2s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=256, nb_epoch=5, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 8s - loss: 6339.4663 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=256, nb_epoch=5, activation=sigmoid, total=  13.8s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=256, nb_epoch=5, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 8s - loss: 5862.2279 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=256, nb_epoch=5, activation=sigmoid, total=  13.1s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Adam'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=256, nb_epoch=10, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 10s - loss: 5891.0890 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Adam'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=256, nb_epoch=10, activation=sigmoid, total=  15.5s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Adam'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=256, nb_epoch=10, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 12s - loss: 6273.8742 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Adam'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=256, nb_epoch=10, activation=sigmoid, total=  19.6s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.Adam'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=256, nb_epoch=10, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 11s - loss: 5798.2174 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.Adam'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=256, nb_epoch=10, activation=sigmoid, total=  16.5s\n",
      "[CV] batch_size=10, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=512, nb_epoch=100, activation=softmax \n",
      "Epoch 1/1\n",
      " - 11s - loss: 5888.9186 - acc: 0.0058\n",
      "[CV]  batch_size=10, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=512, nb_epoch=100, activation=softmax, total=  17.6s\n",
      "[CV] batch_size=10, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=512, nb_epoch=100, activation=softmax \n",
      "Epoch 1/1\n",
      " - 11s - loss: 6272.0639 - acc: 0.0058\n",
      "[CV]  batch_size=10, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=512, nb_epoch=100, activation=softmax, total=  17.0s\n",
      "[CV] batch_size=10, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=512, nb_epoch=100, activation=softmax \n",
      "Epoch 1/1\n",
      " - 11s - loss: 5796.4594 - acc: 0.0058\n",
      "[CV]  batch_size=10, Optimizer=<class 'keras.optimizers.Adagrad'>, out_dim=1, dropout_rate=0.5, learning_rate=5.0, hidden_dim=512, nb_epoch=100, activation=softmax, total=  16.6s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.SGD'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=128, nb_epoch=100, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 8s - loss: 5913.5552 - acc: 0.0116\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.SGD'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=128, nb_epoch=100, activation=sigmoid, total=  13.5s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.SGD'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=128, nb_epoch=100, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 9s - loss: 6294.9383 - acc: 0.0058\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.SGD'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=128, nb_epoch=100, activation=sigmoid, total=  13.9s\n",
      "[CV] batch_size=30, Optimizer=<class 'keras.optimizers.SGD'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=128, nb_epoch=100, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 8s - loss: 5818.1122 - acc: 0.0000e+00\n",
      "[CV]  batch_size=30, Optimizer=<class 'keras.optimizers.SGD'>, out_dim=1, dropout_rate=0.5, learning_rate=0.01, hidden_dim=128, nb_epoch=100, activation=sigmoid, total=  15.9s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.2, learning_rate=0.5, hidden_dim=128, nb_epoch=5, activation=hard_sigmoid \n",
      "Epoch 1/1\n",
      " - 10s - loss: 5890.9770 - acc: 0.0058\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.2, learning_rate=0.5, hidden_dim=128, nb_epoch=5, activation=hard_sigmoid, total=  16.3s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.2, learning_rate=0.5, hidden_dim=128, nb_epoch=5, activation=hard_sigmoid \n",
      "Epoch 1/1\n",
      " - 11s - loss: 6274.2241 - acc: 0.0116\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.2, learning_rate=0.5, hidden_dim=128, nb_epoch=5, activation=hard_sigmoid, total=  17.1s\n",
      "[CV] batch_size=5, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.2, learning_rate=0.5, hidden_dim=128, nb_epoch=5, activation=hard_sigmoid \n",
      "Epoch 1/1\n",
      " - 11s - loss: 5797.5346 - acc: 0.0000e+00\n",
      "[CV]  batch_size=5, Optimizer=<class 'keras.optimizers.RMSprop'>, out_dim=1, dropout_rate=0.2, learning_rate=0.5, hidden_dim=128, nb_epoch=5, activation=hard_sigmoid, total=  17.8s\n",
      "[CV] batch_size=10, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=512, nb_epoch=5, activation=hard_sigmoid \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 74s - loss: 5894.7886 - acc: 0.0058\n",
      "[CV]  batch_size=10, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=512, nb_epoch=5, activation=hard_sigmoid, total= 1.4min\n",
      "[CV] batch_size=10, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=512, nb_epoch=5, activation=hard_sigmoid \n",
      "Epoch 1/1\n",
      " - 12s - loss: 6276.8821 - acc: 0.0058\n",
      "[CV]  batch_size=10, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=512, nb_epoch=5, activation=hard_sigmoid, total=  19.2s\n",
      "[CV] batch_size=10, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=512, nb_epoch=5, activation=hard_sigmoid \n",
      "Epoch 1/1\n",
      " - 12s - loss: 5802.5905 - acc: 0.0058\n",
      "[CV]  batch_size=10, Optimizer=<class 'keras.optimizers.Adadelta'>, out_dim=1, dropout_rate=0.2, learning_rate=5.0, hidden_dim=512, nb_epoch=5, activation=hard_sigmoid, total=  18.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed: 16.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 12s - loss: 13566.0134 - acc: 0.0039\n",
      "High Fit\n",
      "Best: 0.019380 using {'batch_size': 10, 'Optimizer': <class 'keras.optimizers.Adadelta'>, 'out_dim': 1, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'hidden_dim': 128, 'nb_epoch': 10, 'activation': 'relu'}\n",
      "0.000000 (0.000000) with: {'batch_size': 30, 'Optimizer': <class 'keras.optimizers.Nadam'>, 'out_dim': 1, 'dropout_rate': 0.5, 'learning_rate': 5.0, 'hidden_dim': 256, 'nb_epoch': 10, 'activation': 'sigmoid'}\n",
      "0.000000 (0.000000) with: {'batch_size': 5, 'Optimizer': <class 'keras.optimizers.Adadelta'>, 'out_dim': 1, 'dropout_rate': 0.5, 'learning_rate': 0.5, 'hidden_dim': 256, 'nb_epoch': 100, 'activation': 'sigmoid'}\n",
      "0.000000 (0.000000) with: {'batch_size': 30, 'Optimizer': <class 'keras.optimizers.Adamax'>, 'out_dim': 1, 'dropout_rate': 0.5, 'learning_rate': 0.001, 'hidden_dim': 128, 'nb_epoch': 10, 'activation': 'sigmoid'}\n",
      "0.000000 (0.000000) with: {'batch_size': 30, 'Optimizer': <class 'keras.optimizers.Adamax'>, 'out_dim': 1, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'hidden_dim': 128, 'nb_epoch': 10, 'activation': 'sigmoid'}\n",
      "0.000000 (0.000000) with: {'batch_size': 5, 'Optimizer': <class 'keras.optimizers.Adadelta'>, 'out_dim': 1, 'dropout_rate': 0.5, 'learning_rate': 5.0, 'hidden_dim': 256, 'nb_epoch': 10, 'activation': 'softmax'}\n",
      "0.003876 (0.005481) with: {'batch_size': 10, 'Optimizer': <class 'keras.optimizers.Nadam'>, 'out_dim': 1, 'dropout_rate': 0.1, 'learning_rate': 0.5, 'hidden_dim': 128, 'nb_epoch': 10, 'activation': 'linear'}\n",
      "0.000000 (0.000000) with: {'batch_size': 30, 'Optimizer': <class 'keras.optimizers.Adagrad'>, 'out_dim': 1, 'dropout_rate': 0.5, 'learning_rate': 0.5, 'hidden_dim': 256, 'nb_epoch': 5, 'activation': 'hard_sigmoid'}\n",
      "0.003876 (0.005481) with: {'batch_size': 5, 'Optimizer': <class 'keras.optimizers.Adamax'>, 'out_dim': 1, 'dropout_rate': 0.1, 'learning_rate': 0.5, 'hidden_dim': 128, 'nb_epoch': 100, 'activation': 'linear'}\n",
      "0.000000 (0.000000) with: {'batch_size': 10, 'Optimizer': <class 'keras.optimizers.Adagrad'>, 'out_dim': 1, 'dropout_rate': 0.5, 'learning_rate': 0.5, 'hidden_dim': 512, 'nb_epoch': 100, 'activation': 'linear'}\n",
      "0.000000 (0.000000) with: {'batch_size': 10, 'Optimizer': <class 'keras.optimizers.SGD'>, 'out_dim': 1, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'hidden_dim': 256, 'nb_epoch': 100, 'activation': 'tanh'}\n",
      "0.019380 (0.027407) with: {'batch_size': 10, 'Optimizer': <class 'keras.optimizers.Adadelta'>, 'out_dim': 1, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'hidden_dim': 128, 'nb_epoch': 10, 'activation': 'relu'}\n",
      "0.000000 (0.000000) with: {'batch_size': 5, 'Optimizer': <class 'keras.optimizers.Nadam'>, 'out_dim': 1, 'dropout_rate': 0.5, 'learning_rate': 0.01, 'hidden_dim': 128, 'nb_epoch': 100, 'activation': 'tanh'}\n",
      "0.000000 (0.000000) with: {'batch_size': 5, 'Optimizer': <class 'keras.optimizers.Adagrad'>, 'out_dim': 1, 'dropout_rate': 0.5, 'learning_rate': 0.01, 'hidden_dim': 128, 'nb_epoch': 100, 'activation': 'softmax'}\n",
      "0.000000 (0.000000) with: {'batch_size': 30, 'Optimizer': <class 'keras.optimizers.Nadam'>, 'out_dim': 1, 'dropout_rate': 0.1, 'learning_rate': 5.0, 'hidden_dim': 512, 'nb_epoch': 100, 'activation': 'sigmoid'}\n",
      "0.003876 (0.005481) with: {'batch_size': 30, 'Optimizer': <class 'keras.optimizers.Adagrad'>, 'out_dim': 1, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'hidden_dim': 128, 'nb_epoch': 5, 'activation': 'linear'}\n",
      "0.000000 (0.000000) with: {'batch_size': 5, 'Optimizer': <class 'keras.optimizers.Adam'>, 'out_dim': 1, 'dropout_rate': 0.5, 'learning_rate': 0.01, 'hidden_dim': 512, 'nb_epoch': 10, 'activation': 'linear'}\n",
      "0.000000 (0.000000) with: {'batch_size': 30, 'Optimizer': <class 'keras.optimizers.RMSprop'>, 'out_dim': 1, 'dropout_rate': 0.5, 'learning_rate': 0.01, 'hidden_dim': 256, 'nb_epoch': 100, 'activation': 'softmax'}\n",
      "0.000000 (0.000000) with: {'batch_size': 5, 'Optimizer': <class 'keras.optimizers.RMSprop'>, 'out_dim': 1, 'dropout_rate': 0.1, 'learning_rate': 5.0, 'hidden_dim': 128, 'nb_epoch': 10, 'activation': 'linear'}\n",
      "0.000000 (0.000000) with: {'batch_size': 30, 'Optimizer': <class 'keras.optimizers.SGD'>, 'out_dim': 1, 'dropout_rate': 0.5, 'learning_rate': 5.0, 'hidden_dim': 256, 'nb_epoch': 100, 'activation': 'hard_sigmoid'}\n",
      "0.000000 (0.000000) with: {'batch_size': 30, 'Optimizer': <class 'keras.optimizers.Adam'>, 'out_dim': 1, 'dropout_rate': 0.2, 'learning_rate': 0.5, 'hidden_dim': 128, 'nb_epoch': 5, 'activation': 'sigmoid'}\n",
      "0.000000 (0.000000) with: {'batch_size': 5, 'Optimizer': <class 'keras.optimizers.Nadam'>, 'out_dim': 1, 'dropout_rate': 0.5, 'learning_rate': 0.5, 'hidden_dim': 512, 'nb_epoch': 5, 'activation': 'tanh'}\n",
      "0.000000 (0.000000) with: {'batch_size': 5, 'Optimizer': <class 'keras.optimizers.RMSprop'>, 'out_dim': 1, 'dropout_rate': 0.2, 'learning_rate': 5.0, 'hidden_dim': 512, 'nb_epoch': 100, 'activation': 'softmax'}\n",
      "0.000000 (0.000000) with: {'batch_size': 10, 'Optimizer': <class 'keras.optimizers.Adagrad'>, 'out_dim': 1, 'dropout_rate': 0.5, 'learning_rate': 0.01, 'hidden_dim': 512, 'nb_epoch': 10, 'activation': 'sigmoid'}\n",
      "0.000000 (0.000000) with: {'batch_size': 5, 'Optimizer': <class 'keras.optimizers.Adadelta'>, 'out_dim': 1, 'dropout_rate': 0.2, 'learning_rate': 5.0, 'hidden_dim': 256, 'nb_epoch': 100, 'activation': 'tanh'}\n",
      "0.000000 (0.000000) with: {'batch_size': 30, 'Optimizer': <class 'keras.optimizers.Adadelta'>, 'out_dim': 1, 'dropout_rate': 0.5, 'learning_rate': 0.01, 'hidden_dim': 256, 'nb_epoch': 5, 'activation': 'sigmoid'}\n",
      "0.000000 (0.000000) with: {'batch_size': 5, 'Optimizer': <class 'keras.optimizers.Adam'>, 'out_dim': 1, 'dropout_rate': 0.2, 'learning_rate': 5.0, 'hidden_dim': 256, 'nb_epoch': 10, 'activation': 'sigmoid'}\n",
      "0.000000 (0.000000) with: {'batch_size': 10, 'Optimizer': <class 'keras.optimizers.Adagrad'>, 'out_dim': 1, 'dropout_rate': 0.5, 'learning_rate': 5.0, 'hidden_dim': 512, 'nb_epoch': 100, 'activation': 'softmax'}\n",
      "0.000000 (0.000000) with: {'batch_size': 30, 'Optimizer': <class 'keras.optimizers.SGD'>, 'out_dim': 1, 'dropout_rate': 0.5, 'learning_rate': 0.01, 'hidden_dim': 128, 'nb_epoch': 100, 'activation': 'sigmoid'}\n",
      "0.000000 (0.000000) with: {'batch_size': 5, 'Optimizer': <class 'keras.optimizers.RMSprop'>, 'out_dim': 1, 'dropout_rate': 0.2, 'learning_rate': 0.5, 'hidden_dim': 128, 'nb_epoch': 5, 'activation': 'hard_sigmoid'}\n",
      "0.000000 (0.000000) with: {'batch_size': 10, 'Optimizer': <class 'keras.optimizers.Adadelta'>, 'out_dim': 1, 'dropout_rate': 0.2, 'learning_rate': 5.0, 'hidden_dim': 512, 'nb_epoch': 5, 'activation': 'hard_sigmoid'}\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaoru.kitamura\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# データ準備\n",
    "#ここではhigh/lowのtrain/testの４つを整備\n",
    "#配列的にl_of_sとn_next分の前後にとる\n",
    "#X = 入力データ\n",
    "#y = ラベル,教師データ\n",
    "X_high_train, y_high_train = _load_data(data[['high']].iloc[train_start_count-l_of_s:train_end_count+n_next], \\\n",
    "                                        l_of_s, n_next, True)\n",
    "X_high_test , y_high_test  = _load_data(data[['high']].iloc[test_start_count-l_of_s:test_end_count+n_next], \\\n",
    "                                        l_of_s, n_next, True)\n",
    "X_low_train , y_low_train  = _load_data(data[['low']].iloc[train_start_count-l_of_s:train_end_count+n_next], \\\n",
    "                                        l_of_s, n_next, False)\n",
    "X_low_test  , y_low_test   = _load_data(data[['low']].iloc[test_start_count-l_of_s:test_end_count+n_next], \\\n",
    "                                        l_of_s, n_next, False)\n",
    "\n",
    "# 正規化\n",
    "#X_high_train = min_max(X_high_train)\n",
    "#scaler  = MinMaxScaler( feature_range=(0, 1) )    \n",
    "#X_high_train = scaler.fit_transform(X_high_train)\n",
    "#print(\"行列Xの大きさ:\", X_high_train.shape)\n",
    "#print(\"行列Xの行数:\", X_high_train.shape[0])\n",
    "#print(\"行列Xの列数:\", X_high_train.shape[1])\n",
    "\n",
    "#y_high_train = scaler.fit_transform(y_high_train)\n",
    "#print(\"行列yの大きさ:\", y_high_train.shape)\n",
    "#print(\"行列yの行数:\", y_high_train.shape[0])\n",
    "#print(\"行列yの列数:\", y_high_train.shape[1])\n",
    "\n",
    "X_high_train = (X_high_train-min_value)/(max_value-min_value)\n",
    "y_high_train = (y_high_train-min_value)/(max_value-min_value)\n",
    "X_high_test = (X_high_test-min_value)/(max_value-min_value)\n",
    "y_high_test = (y_high_test-min_value)/(max_value-min_value)\n",
    "X_low_train = (X_low_train-min_value)/(max_value-min_value)\n",
    "y_low_train = (y_low_train-min_value)/(max_value-min_value)\n",
    "X_low_test = (X_low_test-min_value)/(max_value-min_value)\n",
    "y_low_test = (y_low_test-min_value)/(max_value-min_value)\n",
    "\n",
    "\n",
    "#####################\n",
    "# テストパターンの定義 #\n",
    "#####################\n",
    "# 活性化関数\n",
    "activation = [\"relu\", \"tanh\", \"sigmoid\", \"hard_sigmoid\", \"linear\", \"softmax\"]\n",
    "# 最適化アルゴリズム\n",
    "optimizer = [SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam]\n",
    "# 出力層のユニット数\n",
    "out_dim = [1]\n",
    "# 隠れ層のユニット数\n",
    "hidden_dim = [128, 256, 512]\n",
    "# エポック数\n",
    "nb_epoch = [5, 10, 100]\n",
    "# バッチサイズ\n",
    "batch_size = [5, 10, 30]\n",
    "# 学習率\n",
    "learning_rate = [0.001, 0.01, 0.5, 5.0]\n",
    "# ドロップアウト率\n",
    "dropout_rate = [0.1, 0.2, 0.5]\n",
    "\n",
    "\n",
    "# EarlyStopping定義\n",
    "early_stopping = EarlyStopping(patience=0, verbose=1)\n",
    "\n",
    "# ニューラルネットの定義\n",
    "model = KerasClassifier(build_fn=lstm_model, verbose=2)\n",
    "# テストパターンの設定\n",
    "param_grid = dict(activation=activation, \n",
    "                  Optimizer=optimizer, \n",
    "                  out_dim=out_dim,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  nb_epoch=nb_epoch, \n",
    "                  batch_size=batch_size,\n",
    "                  learning_rate=learning_rate,\n",
    "                  dropout_rate=dropout_rate\n",
    "                 )\n",
    "# グリッドサーチ\n",
    "#grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "#result = grid.fit(X_high_train, y_high_train)\n",
    "\n",
    "# ランダムサーチ\n",
    "# サーチ回数\n",
    "n_iter_search = 30\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, \n",
    "                                   n_iter=n_iter_search, n_jobs=1, verbose=2)\n",
    "result=random_search.fit(X_high_train, y_high_train, callbacks=[early_stopping])\n",
    "\n",
    "# 学習\n",
    "print(\"High Fit\")\n",
    "\n",
    "# テスト結果表示\n",
    "print(\"Best: %f using %s\" % (result.best_score_, result.best_params_))\n",
    "means = result.cv_results_['mean_test_score']\n",
    "stds = result.cv_results_['std_test_score']\n",
    "params = result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "sys.exit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2018/1/24 Kitamura Add Start\n",
    "# 結果のプロット\n",
    "#high_result = pandas.DataFrame(high_predicted)\n",
    "#high_result.columns = ['predict']\n",
    "#high_result['actual'] = y_high_test\n",
    "#high_result.plot()\n",
    "#plt.show()\n",
    "# 2018/1/24 Kitamura Add End\n",
    "\n",
    "print(\"Low Fit\")\n",
    "# 学習\n",
    "X_low_train = X_low_train[ len(X_low_train) % batch_size: ]\n",
    "low_model.fit(X_low_train, y_low_train, batch_size=batch_size, epochs=100, validation_split=0.0)\n",
    "\n",
    "# テスト結果表示\n",
    "low_predicted = low_model.predict(X_low_test)\n",
    "\n",
    "# 2018/3/3 Kitamura add Start\n",
    "#正規化\n",
    "y_low_test = scaler.fit_transform(y_low_test)\n",
    "# 2018/3/3 Kitamura add End\n",
    "\n",
    "# 2018/1/24 Kitamura Add Start\n",
    "# 結果のプロット\n",
    "low_result = pandas.DataFrame(low_predicted)\n",
    "low_result.columns = ['predict']\n",
    "low_result['actual'] = y_low_test\n",
    "low_result.plot()\n",
    "plt.show()\n",
    "# 2018/1/24 Kitamura Add End\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#high/lowの予想最大/最小値のグラフ（小さくてわからない）\n",
    "#result = pandas.DataFrame(high_predicted)\n",
    "#result.columns = ['high_predict']\n",
    "#result['low_predict'] = low_predicted\n",
    "#result.plot()\n",
    "#plt.show()\n",
    "\n",
    "#\n",
    "pre_check = []\n",
    "temp_check = []\n",
    "temp_close_open = []\n",
    "temp_close_open_up_win = []\n",
    "temp_close_open_up_lost = []\n",
    "temp_close_open_down_win = []\n",
    "temp_close_open_down_lost = []\n",
    "\n",
    "#使わないデータも保持しているが\n",
    "#予想したHighの最大値とLowの最小値を起点の日の翌日のOpenの\n",
    "#と比較し、最初sに決定した差よりも大きい場合は上がり・下がりと\n",
    "#判断する\n",
    "#ただし、同時超えた場合はどちらが先に上がるか不明なためカウントしていない\n",
    "#もしかすると\n",
    "for i in range(len(low_predicted)):\n",
    "    high_temp = high_predicted[i] * diff_value + average_value\n",
    "    low_temp  = low_predicted[i] * diff_value + average_value\n",
    "    open_temp = data.loc[i+test_start_count+1, 'open'] * diff_value + average_value\n",
    "    close_temp = data.loc[i+test_start_count, 'close'] * diff_value + average_value\n",
    "\n",
    "    if high_temp - close_temp >= check_treshhold and \\\n",
    "        close_temp - low_temp < check_treshhold:\n",
    "        pre_check.append(1)\n",
    "        temp_check.append(high_temp - open_temp)\n",
    "        temp_close_open.append(abs(close_temp - open_temp))\n",
    "\n",
    "    elif close_temp- low_temp >= check_treshhold and \\\n",
    "        high_temp - close_temp < check_treshhold:\n",
    "        pre_check.append(-1)\n",
    "        temp_check.append(low_temp - open_temp)\n",
    "        temp_close_open.append(abs(close_temp - open_temp))\n",
    "    #elif high_temp - open_temp >= check_treshhold and \\\n",
    "    #    open_temp- low_temp >= check_treshhold:\n",
    "        \n",
    "    #    if high_temp - open_temp > open_temp- low_temp:\n",
    "    #        pre_check.append(1)\n",
    "    #        temp_check.append(high_temp - open_temp)\n",
    "    #    elif high_temp - open_temp < open_temp- low_temp:\n",
    "    #        pre_check.append(-1)\n",
    "    #        temp_check.append(low_temp - open_temp)\n",
    "    #    else:\n",
    "    #        pre_check.append(0)\n",
    "    #        temp_check.append(0)\n",
    "\n",
    "    #if high_temp - open_temp >= check_treshhold and \\\n",
    "    #    open_temp - low_temp < check_treshhold and \\\n",
    "    #    close_temp - open_temp <= up_c_o_diff and \\\n",
    "    #    close_temp - open_temp >= down_c_o_diff:\n",
    "    #    pre_check.append(1)\n",
    "    #    temp_check.append(high_temp - open_temp)\n",
    "\n",
    "    #elif open_temp- low_temp >= check_treshhold and \\\n",
    "    #    high_temp - open_temp < check_treshhold and \\\n",
    "    #    close_temp - open_temp <= up_c_o_diff and \\\n",
    "    #    close_temp - open_temp >= down_c_o_diff:\n",
    "    #    pre_check.append(-1)\n",
    "    #    temp_check.append(low_temp - open_temp)\n",
    "\n",
    "    else:\n",
    "        pre_check.append(0)\n",
    "        temp_check.append(0)\n",
    "\n",
    "up_ok_count = 0\n",
    "up_ng_count = 0\n",
    "up_ev_count = 0\n",
    "down_ok_count = 0\n",
    "down_ng_count = 0\n",
    "down_ev_count = 0\n",
    "high_win = numpy.array([])\n",
    "high_lost = numpy.array([])\n",
    "low_win  = numpy.array([])\n",
    "low_lost  = numpy.array([])\n",
    "\n",
    "for i in range(len(pre_check)):\n",
    "    if pre_check[i] == 1:\n",
    "        if up_down[i] == pre_check[i]:\n",
    "            up_ok_count += 1\n",
    "            high_win = numpy.append(high_win, numpy.array(temp_check[i]))\n",
    "        elif up_down[i] != pre_check[i] and up_down[i] == -1:\n",
    "            up_ng_count += 1\n",
    "            high_lost = numpy.append(high_lost, numpy.array(temp_check[i]))\n",
    "        else:\n",
    "            up_ev_count += 1\n",
    "            \n",
    "    elif pre_check[i] == -1:\n",
    "        if up_down[i] == pre_check[i]:\n",
    "            down_ok_count += 1\n",
    "            low_win = numpy.append(low_win, numpy.array(temp_check[i]))\n",
    "        elif up_down[i] != pre_check[i] and up_down[i] == 1:\n",
    "            down_ng_count += 1\n",
    "            low_lost = numpy.append(low_lost, numpy.array(temp_check[i]))\n",
    "        else:\n",
    "            down_ev_count += 1\n",
    "\n",
    "\n",
    "print('==========')\n",
    "print('UP:')\n",
    "print(' WIN  :' + str(up_ok_count))\n",
    "print(' LOST :' + str(up_ng_count))\n",
    "print(' DRAW :' + str(up_ev_count))\n",
    "print(' WIN RATE :' + str(math.floor((up_ok_count/(up_ok_count+up_ng_count))*100)) + '%')\n",
    "print('DOWN:')\n",
    "print(' WIN  :' + str(down_ok_count))\n",
    "print(' LOST :' + str(down_ng_count))\n",
    "print(' DRAW :' + str(down_ev_count))\n",
    "print(' WIN RATE :' + str(math.floor((down_ok_count/(down_ok_count+down_ng_count))*100)) + '%')\n",
    "#print('---------')\n",
    "#print('UP ')\n",
    "#print('  WIN  :' + str(high_win.mean()) + ' ' + str(high_win.std()))\n",
    "#print('  LOST  :' + str(high_lost.mean()) + ' ' + str(high_lost.std()))\n",
    "#print('DOWN ')\n",
    "#print('  WIN  :' + str(low_win.mean()) + ' ' + str(low_win.std()))\n",
    "#print('  LOST  :' + str(low_lost.mean()) + ' ' + str(low_lost.std()))\n",
    "\n",
    "\n",
    "print(str(math.floor(time.time() - starttime)) + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
