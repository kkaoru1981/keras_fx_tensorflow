{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "import math\n",
    "# 2018/3/3 Kitamura Add Start\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# 2018/3/3 Kitamura Add End\n",
    "\n",
    "#スタート時間を保持\n",
    "starttime = time.time()\n",
    "\n",
    "# 学習の設定\n",
    "l_of_s         = 10\n",
    "n_next         = 3\n",
    "in_out_neurons = 1\n",
    "hidden_neurons = 200\n",
    "batch_size = 10\n",
    "\n",
    "# 学習用データを抽出する関数\n",
    "def _load_data(data, n_prev=10, n_next=1, flag=True):\n",
    "    docX, docY = [], []\n",
    "    for i in range(n_prev, len(data)-n_next):\n",
    "        #起点の箇所からn_prevだけ戻った日数分をデータとする\n",
    "        docX.append(data.iloc[i-n_prev:i].as_matrix())\n",
    "        #起点の翌日からn_next日数分進んだデータのmax or minをyデータとする\n",
    "        if flag == True:\n",
    "            docY.append(max(data.iloc[i+1:i+n_next].as_matrix()))\n",
    "        else:\n",
    "            docY.append(min(data.iloc[i+1:i+n_next].as_matrix()))\n",
    "\n",
    "    alsX = numpy.array(docX)\n",
    "    alsY = numpy.array(docY)\n",
    "    alsX\n",
    "    alsY\n",
    "    return alsX, alsY\n",
    "\n",
    "def lstm_model(activation=\"relu\", optimizer=\"adam\", out_dim=1):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_neurons, \\\n",
    "          batch_input_shape=(None, l_of_s, in_out_neurons), \\\n",
    "          return_sequences=False))\n",
    "    model.add(Dense(out_dim))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def min_max(x, axis=None):\n",
    "    min = x.min(axis=axis, keepdims=True)\n",
    "    max = x.max(axis=axis, keepdims=True)\n",
    "    result = (x-min)/(max-min)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('usdjpy_d.csv', <http.client.HTTPMessage at 0x4c63b00>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "#ダウンロードする通貨ペア\n",
    "#https://stooq.com/q/d/?s=usdjpy\n",
    "#ついでにsekjpy、nokjpy、mxnjpy、sgdjpyとかもありますが、\n",
    "#2000/01/01からデータがない（あるけど、しばらくopen/high/low/closeがすべて一緒)ので\n",
    "#ここには７通貨ペアしか記載しておりません。日付とか調整して通貨ペアを増やしてみても\n",
    "#いいと思います\n",
    "currency_pair = 'usdjpy'\n",
    "#currency_pair = 'eurjpy'\n",
    "#currency_pair = 'gbpjpy'\n",
    "#currency_pair = 'audjpy'\n",
    "#currency_pair = 'cadjpy'\n",
    "#currency_pair = 'chfjpy'\n",
    "#currency_pair = 'nzdjpy'\n",
    "#currency_pair = 'sekjpy'\n",
    "#currency_pair = 'nokjpy'\n",
    "\n",
    "#スタート日付\n",
    "start_day     = \"20150101\"\n",
    "#終了日を今日に指定\n",
    "url           = \"https://stooq.com/q/d/l/?s=\" + currency_pair + \\\n",
    "                \"&d1=\" + start_day + \"&d2=\" + dt.today().strftime(\"%Y%m%d\") + \"&i=d\"\n",
    "file_name     = currency_pair + '_d.csv'\n",
    "#取得して、ファイルに保存(よくよく考えると保存しなくてもいいな)\n",
    "urllib.request.urlretrieve(url, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Start: 259\n",
      "Train End  : 517\n",
      "Test  Start: 517\n",
      "Test  End  : 776\n",
      "(U+D)/(U+D+E): 35%\n",
      "(U+D)/(U+D+E): 37%\n",
      "(U+D)/(U+D+E): 40%\n",
      "(U+D)/(U+D+E): 43%\n",
      "(U+D)/(U+D+E): 45%\n",
      "(U+D)/(U+D+E): 49%\n",
      "(U+D)/(U+D+E): 51%\n",
      "(U+D)/(U+D+E): 54%\n",
      "(U+D)/(U+D+E): 56%\n",
      "(U+D)/(U+D+E): 60%\n",
      "(U+D)/(U+D+E): 63%\n",
      "(U+D)/(U+D+E): 64%\n",
      "(U+D)/(U+D+E): 66%\n",
      "p            : 0.4600000000000002\n",
      "UP   COUNT   : 156\n",
      "DOWN COUNT   : 189\n",
      "EVNE COUNT   : 172\n",
      "(U+D)/(U+D+E): 66%\n",
      "max: 125.85\n",
      "min: 99.101\n",
      "ave: 112.4755\n",
      "dif: 13.374499999999998\n"
     ]
    }
   ],
   "source": [
    "# FXデータの読み込み\n",
    "data = None\n",
    "data = pandas.read_csv(file_name)\n",
    "data.columns = ['date', 'open', 'high', 'low', 'close']\n",
    "data['date'] = pandas.to_datetime(data['date'], format='%Y-%m-%d')\n",
    "\n",
    "#train開始終了、test開始終了日を設定\n",
    "#データは2001/01/01からとっていますが、さかのぼって調査する関係上\n",
    "#20001/01/01からにしています。また買い目対象の調査としてtest_end_dayよりも\n",
    "#後の日も参照するためtest_end_dayの取得するデータにはしないでください。\n",
    "#trainの終了日とtestの開始日が連続していますが、連続させなくても動作するか\n",
    "#検証していません。注意してください\n",
    "train_start_day   = dt.strptime('2016-01-01', '%Y-%m-%d')\n",
    "train_end_day     = dt.strptime('2016-12-31', '%Y-%m-%d')\n",
    "test_start_day    = dt.strptime('2017-01-01', '%Y-%m-%d')\n",
    "test_end_day      = dt.strptime('2017-12-31', '%Y-%m-%d')\n",
    "train_start_count = -1\n",
    "train_end_count   = -1\n",
    "test_start_count  = -1\n",
    "test_end_count    = -1\n",
    "\n",
    "#train/testの開始終了日の配列の場所を調査\n",
    "for i in range(len(data['date'])):\n",
    "    if train_start_count == -1 and data['date'][i] >= train_start_day:\n",
    "        train_start_count = i\n",
    "    if train_end_count == -1 and data['date'][i] >= train_end_day:\n",
    "        train_end_count = i\n",
    "    if test_start_count == -1 and data['date'][i] >= test_start_day:\n",
    "        test_start_count = i\n",
    "    if test_end_count == -1 and data['date'][i] >= test_end_day:\n",
    "        test_end_count = i\n",
    "        break\n",
    "\n",
    "#前にl_of_s日分、後ろにn_next分日数が必要なので\n",
    "#チェック。足りない場合は中止。これを考慮に入れて\n",
    "#train/testの開始終了日を設定してください\n",
    "if train_start_count - l_of_s < 0 or \\\n",
    "    test_end_count + n_next > len(data['date']):\n",
    "    \n",
    "    print(\"data range over\")\n",
    "    sys.exit()\n",
    "    \n",
    "print('Train Start: ' + str(train_start_count))\n",
    "print('Train End  : ' + str(train_end_count))\n",
    "print('Test  Start: ' + str(test_start_count))\n",
    "print('Test  End  : ' + str(test_end_count))\n",
    "\n",
    "up_down           = []\n",
    "up_count          = 0\n",
    "down_count        = 0\n",
    "even_count        = 0\n",
    "#up/downの割合\n",
    "check_treshhold   = 0.666\n",
    "loop_flag         = True\n",
    "\n",
    "#\n",
    "check_add_percent = 0.0020\n",
    "check_percent     = 0.02\n",
    "if min(data.loc[:, 'low']) > 150:\n",
    "    check_add_percent = 0.20\n",
    "    check_percent     = 2.0\n",
    "elif min(data.loc[:, 'low']) > 30:\n",
    "    check_add_percent = 0.020\n",
    "    check_percent     = 0.2\n",
    "    \n",
    "#close_open_diff = numpy.array([])\n",
    "#for i in range(train_start_count, test_end_count):\n",
    "#    close_open_diff = numpy.append(close_open_diff, numpy.array(data.loc[i-1, 'close'] - data.loc[i, 'open']))\n",
    "#print('Close-Open Diff:' + str(close_open_diff.mean()) + ' ' + str(close_open_diff.std()))\n",
    "#up_c_o_diff   = close_open_diff.mean() + close_open_diff.std() * 2\n",
    "#down_c_o_diff = close_open_diff.mean() - close_open_diff.std() * 2\n",
    "#c_o_d_remove_count = 0\n",
    "#for i in range(train_start_count, test_end_count):\n",
    "#    if data.loc[i-1, 'close'] - data.loc[i, 'open'] >= up_c_o_diff or \\\n",
    "#        data.loc[i-1, 'close'] - data.loc[i, 'open'] <= down_c_o_diff:\n",
    "#        c_o_d_remove_count += 1\n",
    "#print(\"c_o_d_remove_count:\" + str(c_o_d_remove_count))\n",
    "\n",
    "while loop_flag:\n",
    "    up_count = 0\n",
    "    down_count = 0\n",
    "    even_count = 0\n",
    "    check_percent += check_add_percent\n",
    "    for i in range(train_start_count, test_end_count):\n",
    "        #起点の日の翌日のopenの値\n",
    "        open_value = data.loc[i+1, 'open']\n",
    "        #起点の日から翌日からn_next日数分のhighの最大値\n",
    "        max_value = max(data.loc[i+1:i+n_next, 'high'])\n",
    "        #起点の日から翌日からn_next日数分のlowの最小値\n",
    "        min_value = min(data.loc[i+1:i+n_next, 'low'])\n",
    "        #ここは起点の日の翌日のopenの値と起点の日の翌日から\n",
    "        #n_next日数分のhighの最高値かlowの最小値が規定の値上に\n",
    "        #差が広がったカウントを調べる。これで上がった・下がったを\n",
    "        #回数を調べる\n",
    "        \n",
    "        if abs(max_value - open_value) >= check_percent and \\\n",
    "            abs(open_value - min_value) < check_percent:\n",
    "            up_count += 1\n",
    "        elif abs(open_value - min_value) >= check_percent and \\\n",
    "            abs(max_value - open_value) < check_percent:\n",
    "            down_count += 1  \n",
    "        else:\n",
    "            even_count += 1\n",
    "    \n",
    "    #(上がった日+下がった日)/全体の日数でcheck_treshholdを超えたか調べる\n",
    "    #超えていればその値をベースにする\n",
    "    print('(U+D)/(U+D+E): ' + str(math.floor((up_count + down_count) / \\\n",
    "                                             (up_count + down_count +even_count) * 100)) + '%')    \n",
    "    if (up_count + down_count) / (up_count + down_count +even_count) > check_treshhold:\n",
    "        break\n",
    "\n",
    "#上がった・下がったの判定するための数字（もっと言うと円がどれだけ動いたか）\n",
    "print('p            : ' + str(check_percent))\n",
    "print(\"UP   COUNT   : \" + str(up_count))\n",
    "print(\"DOWN COUNT   : \" + str(down_count))\n",
    "print(\"EVNE COUNT   : \" + str(even_count))\n",
    "print('(U+D)/(U+D+E): ' + str(math.floor((up_count + down_count) / \\\n",
    "                                         (up_count + down_count +even_count) * 100)) + '%')    \n",
    "\n",
    "#sys.exit()\n",
    "#上がった・下がったの判定用\n",
    "#ここではtestの範囲のみ取得\n",
    "for i in range(test_start_count, test_end_count):\n",
    "    open_value = data.loc[i+1, 'open']\n",
    "    close_value = data.loc[i, 'open']\n",
    "    #起点の日から翌日からn_next日数分のhighの最大値\n",
    "    max_value = max(data.loc[i+1:i+n_next, 'high'])\n",
    "    #起点の日から翌日からn_next日数分のlowの最小値\n",
    "    min_value = min(data.loc[i+1:i+n_next, 'low'])\n",
    "    #ここは起点の日の翌日のopenの値と起点の日の翌日から\n",
    "    #n_next日数分のhighの最高値かlowの最小値が規定の値上に\n",
    "    #差が広がったカウントを調べる。これで上がった・下がったを\n",
    "    #回数を調べる\n",
    "    if abs(max_value - open_value) >= check_percent and \\\n",
    "        abs(open_value - min_value) < check_percent:\n",
    "        up_down.append(1)\n",
    "    elif abs(open_value - min_value) >= check_percent and \\\n",
    "        abs(max_value - open_value) < check_percent:\n",
    "        up_down.append(-1)\n",
    "    else:\n",
    "        up_down.append(0)\n",
    "\n",
    "max_value = max(data['high'])\n",
    "min_value = min(data['low'])\n",
    "average_value = (max_value+min_value)/2\n",
    "diff_value = max_value - average_value\n",
    "print ('max: ' + str(max_value))\n",
    "print ('min: ' + str(min_value))\n",
    "print ('ave: ' + str(average_value))\n",
    "print ('dif: ' + str(diff_value))\n",
    "\n",
    "#for i in range(len(data['high'].index)):\n",
    "#data.loc[i, 'high'] = (data.loc[i, 'high'] - average_value) / diff_value\n",
    "#data.loc[i, 'low']  = (data.loc[i, 'low'] - average_value) / diff_value\n",
    "#data.loc[i, 'open'] = (data.loc[i, 'open'] - average_value) / diff_value\n",
    "#data.loc[i, 'close'] = (data.loc[i, 'close'] - average_value) / diff_value\n",
    "\n",
    "# 2018/3/3 Kitamura add Start\n",
    "# 正規化\n",
    "#scaler  = MinMaxScaler( feature_range=(0, 1) )    \n",
    "#data.loc[i, 'high'] = scaler.fit_transform(data.loc[i, 'high'])\n",
    "#data.loc[i, 'low'] = scaler.fit_transform(data.loc[i, 'low'])\n",
    "#data.loc[i, 'open'] = scaler.fit_transform(data.loc[i, 'open'])\n",
    "#data.loc[i, 'close'] = scaler.fit_transform(data.loc[i, 'close'])\n",
    "# 2018/3/3 Kitamura add End\n",
    "\n",
    "data = data.sort_values(by='date')\n",
    "data = data.reset_index(drop=True)\n",
    "data = data.loc[:, ['date', 'open','high', 'low', 'close']]\n",
    "\n",
    "#グラフ化\n",
    "#plt.plot(data['date'], data['high'])\n",
    "#plt.plot(data['date'], data['low'])\n",
    "#plt.show()\n",
    "\n",
    "#sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行列Xの大きさ: (258, 10, 1)\n",
      "行列Xの行数: 258\n",
      "行列Xの列数: 10\n",
      "行列yの大きさ: (258, 1)\n",
      "行列yの行数: 258\n",
      "行列yの列数: 1\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] out_dim=1, optimizer=adagrad, nb_epoch=10, batch_size=10, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 1s - loss: -3.7730e+02 - acc: 0.0058\n",
      "[CV]  out_dim=1, optimizer=adagrad, nb_epoch=10, batch_size=10, activation=sigmoid, total=   3.6s\n",
      "[CV] out_dim=1, optimizer=adagrad, nb_epoch=10, batch_size=10, activation=sigmoid \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 1s - loss: -3.9433e+02 - acc: 0.0058\n",
      "[CV]  out_dim=1, optimizer=adagrad, nb_epoch=10, batch_size=10, activation=sigmoid, total=   3.1s\n",
      "[CV] out_dim=1, optimizer=adagrad, nb_epoch=10, batch_size=10, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 1s - loss: -4.0166e+02 - acc: 0.0058\n",
      "[CV]  out_dim=1, optimizer=adagrad, nb_epoch=10, batch_size=10, activation=sigmoid, total=   3.0s\n",
      "[CV] out_dim=1, optimizer=adagrad, nb_epoch=100, batch_size=5, activation=relu \n",
      "Epoch 1/1\n",
      " - 2s - loss: -4.7802e+02 - acc: 0.0058\n",
      "[CV]  out_dim=1, optimizer=adagrad, nb_epoch=100, batch_size=5, activation=relu, total=   3.5s\n",
      "[CV] out_dim=1, optimizer=adagrad, nb_epoch=100, batch_size=5, activation=relu \n",
      "Epoch 1/1\n",
      " - 2s - loss: -5.0911e+02 - acc: 0.0058\n",
      "[CV]  out_dim=1, optimizer=adagrad, nb_epoch=100, batch_size=5, activation=relu, total=   3.7s\n",
      "[CV] out_dim=1, optimizer=adagrad, nb_epoch=100, batch_size=5, activation=relu \n",
      "Epoch 1/1\n",
      " - 2s - loss: -4.4801e+02 - acc: 0.0058\n",
      "[CV]  out_dim=1, optimizer=adagrad, nb_epoch=100, batch_size=5, activation=relu, total=   3.5s\n",
      "[CV] out_dim=1, optimizer=adam, nb_epoch=10, batch_size=32, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 1s - loss: -9.9634e+01 - acc: 0.0058\n",
      "[CV]  out_dim=1, optimizer=adam, nb_epoch=10, batch_size=32, activation=sigmoid, total=   3.7s\n",
      "[CV] out_dim=1, optimizer=adam, nb_epoch=10, batch_size=32, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 2s - loss: 10.2844 - acc: 0.0058\n",
      "[CV]  out_dim=1, optimizer=adam, nb_epoch=10, batch_size=32, activation=sigmoid, total=   3.7s\n",
      "[CV] out_dim=1, optimizer=adam, nb_epoch=10, batch_size=32, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 2s - loss: -4.3548e+00 - acc: 0.0116\n",
      "[CV]  out_dim=1, optimizer=adam, nb_epoch=10, batch_size=32, activation=sigmoid, total=   3.8s\n",
      "[CV] out_dim=1, optimizer=adam, nb_epoch=10, batch_size=5, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 2s - loss: -1.3830e+02 - acc: 0.0058\n",
      "[CV]  out_dim=1, optimizer=adam, nb_epoch=10, batch_size=5, activation=sigmoid, total=   4.5s\n",
      "[CV] out_dim=1, optimizer=adam, nb_epoch=10, batch_size=5, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 2s - loss: -2.0091e+02 - acc: 0.0058\n",
      "[CV]  out_dim=1, optimizer=adam, nb_epoch=10, batch_size=5, activation=sigmoid, total=   4.2s\n",
      "[CV] out_dim=1, optimizer=adam, nb_epoch=10, batch_size=5, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 2s - loss: -1.3576e+02 - acc: 0.0116\n",
      "[CV]  out_dim=1, optimizer=adam, nb_epoch=10, batch_size=5, activation=sigmoid, total=   5.1s\n",
      "[CV] out_dim=1, optimizer=adam, nb_epoch=10, batch_size=10, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 2s - loss: -1.1680e+02 - acc: 0.0058\n",
      "[CV]  out_dim=1, optimizer=adam, nb_epoch=10, batch_size=10, activation=sigmoid, total=   4.7s\n",
      "[CV] out_dim=1, optimizer=adam, nb_epoch=10, batch_size=10, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 2s - loss: -1.6134e+02 - acc: 0.0058\n",
      "[CV]  out_dim=1, optimizer=adam, nb_epoch=10, batch_size=10, activation=sigmoid, total=   4.9s\n",
      "[CV] out_dim=1, optimizer=adam, nb_epoch=10, batch_size=10, activation=sigmoid \n",
      "Epoch 1/1\n",
      " - 2s - loss: -2.0741e+02 - acc: 0.0058\n",
      "[CV]  out_dim=1, optimizer=adam, nb_epoch=10, batch_size=10, activation=sigmoid, total=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 2s - loss: -6.9723e+02 - acc: 0.0039\n",
      "High Fit\n",
      "Best: 0.000000 using {'out_dim': 1, 'optimizer': 'adagrad', 'nb_epoch': 10, 'batch_size': 10, 'activation': 'sigmoid'}\n",
      "0.000000 (0.000000) with: {'out_dim': 1, 'optimizer': 'adagrad', 'nb_epoch': 10, 'batch_size': 10, 'activation': 'sigmoid'}\n",
      "0.000000 (0.000000) with: {'out_dim': 1, 'optimizer': 'adagrad', 'nb_epoch': 100, 'batch_size': 5, 'activation': 'relu'}\n",
      "0.000000 (0.000000) with: {'out_dim': 1, 'optimizer': 'adam', 'nb_epoch': 10, 'batch_size': 32, 'activation': 'sigmoid'}\n",
      "0.000000 (0.000000) with: {'out_dim': 1, 'optimizer': 'adam', 'nb_epoch': 10, 'batch_size': 5, 'activation': 'sigmoid'}\n",
      "0.000000 (0.000000) with: {'out_dim': 1, 'optimizer': 'adam', 'nb_epoch': 10, 'batch_size': 10, 'activation': 'sigmoid'}\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kaoru\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# データ準備\n",
    "#ここではhigh/lowのtrain/testの４つを整備\n",
    "#配列的にl_of_sとn_next分の前後にとる\n",
    "#X = 入力データ\n",
    "#y = ラベル,教師データ\n",
    "X_high_train, y_high_train = _load_data(data[['high']].iloc[train_start_count-l_of_s:train_end_count+n_next], \\\n",
    "                                        l_of_s, n_next, True)\n",
    "X_high_test , y_high_test  = _load_data(data[['high']].iloc[test_start_count-l_of_s:test_end_count+n_next], \\\n",
    "                                        l_of_s, n_next, True)\n",
    "X_low_train , y_low_train  = _load_data(data[['low']].iloc[train_start_count-l_of_s:train_end_count+n_next], \\\n",
    "                                        l_of_s, n_next, False)\n",
    "X_low_test  , y_low_test   = _load_data(data[['low']].iloc[test_start_count-l_of_s:test_end_count+n_next], \\\n",
    "                                        l_of_s, n_next, False)\n",
    "\n",
    "# 正規化\n",
    "#X_high_train = min_max(X_high_train)\n",
    "#scaler  = MinMaxScaler( feature_range=(0, 1) )    \n",
    "#X_high_train = scaler.fit_transform(X_high_train)\n",
    "print(\"行列Xの大きさ:\", X_high_train.shape)\n",
    "print(\"行列Xの行数:\", X_high_train.shape[0])\n",
    "print(\"行列Xの列数:\", X_high_train.shape[1])\n",
    "\n",
    "#y_high_train = scaler.fit_transform(y_high_train)\n",
    "print(\"行列yの大きさ:\", y_high_train.shape)\n",
    "print(\"行列yの行数:\", y_high_train.shape[0])\n",
    "print(\"行列yの列数:\", y_high_train.shape[1])\n",
    "\n",
    "# ニューラルネットの定義\n",
    "#LSTMの定義/high/lowの両方を別々に保持したけど\n",
    "#modeは共用してfitだけ違えればいい気がしないでもない\n",
    "activation = [\"relu\", \"tanh\", \"sigmoid\"]\n",
    "optimizer = [\"adam\", \"adagrad\"]\n",
    "#optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "out_dim = [1]\n",
    "nb_epoch = [10, 100]\n",
    "batch_size = [5, 10]\n",
    "#dropout_rate = [0.0, 0.5, 0.9]\n",
    "#learn_rate = [0.001, 0.01, 0.1]\n",
    "\n",
    "model = KerasClassifier(build_fn=lstm_model, verbose=2)\n",
    "param_grid = dict(activation=activation, \n",
    "                  optimizer=optimizer, \n",
    "                  out_dim=out_dim, \n",
    "                  nb_epoch=nb_epoch, \n",
    "                  batch_size=batch_size)\n",
    "# グリッドサーチ\n",
    "#grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "#result = grid.fit(X_high_train, y_high_train)\n",
    "\n",
    "# ランダムサーチ\n",
    "n_iter_search = 5\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, \n",
    "                                   n_iter=n_iter_search, n_jobs=1, verbose=2)\n",
    "result=random_search.fit(X_high_train, y_high_train)\n",
    "\n",
    "# 学習\n",
    "print(\"High Fit\")\n",
    "\n",
    "# テスト結果表示\n",
    "print(\"Best: %f using %s\" % (result.best_score_, result.best_params_))\n",
    "means = result.cv_results_['mean_test_score']\n",
    "stds = result.cv_results_['std_test_score']\n",
    "params = result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "sys.exit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2018/1/24 Kitamura Add Start\n",
    "# 結果のプロット\n",
    "#high_result = pandas.DataFrame(high_predicted)\n",
    "#high_result.columns = ['predict']\n",
    "#high_result['actual'] = y_high_test\n",
    "#high_result.plot()\n",
    "#plt.show()\n",
    "# 2018/1/24 Kitamura Add End\n",
    "\n",
    "print(\"Low Fit\")\n",
    "# 学習\n",
    "X_low_train = X_low_train[ len(X_low_train) % batch_size: ]\n",
    "low_model.fit(X_low_train, y_low_train, batch_size=batch_size, epochs=100, validation_split=0.0)\n",
    "\n",
    "# テスト結果表示\n",
    "low_predicted = low_model.predict(X_low_test)\n",
    "\n",
    "# 2018/3/3 Kitamura add Start\n",
    "#正規化\n",
    "y_low_test = scaler.fit_transform(y_low_test)\n",
    "# 2018/3/3 Kitamura add End\n",
    "\n",
    "# 2018/1/24 Kitamura Add Start\n",
    "# 結果のプロット\n",
    "low_result = pandas.DataFrame(low_predicted)\n",
    "low_result.columns = ['predict']\n",
    "low_result['actual'] = y_low_test\n",
    "low_result.plot()\n",
    "plt.show()\n",
    "# 2018/1/24 Kitamura Add End\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#high/lowの予想最大/最小値のグラフ（小さくてわからない）\n",
    "#result = pandas.DataFrame(high_predicted)\n",
    "#result.columns = ['high_predict']\n",
    "#result['low_predict'] = low_predicted\n",
    "#result.plot()\n",
    "#plt.show()\n",
    "\n",
    "#\n",
    "pre_check = []\n",
    "temp_check = []\n",
    "temp_close_open = []\n",
    "temp_close_open_up_win = []\n",
    "temp_close_open_up_lost = []\n",
    "temp_close_open_down_win = []\n",
    "temp_close_open_down_lost = []\n",
    "\n",
    "#使わないデータも保持しているが\n",
    "#予想したHighの最大値とLowの最小値を起点の日の翌日のOpenの\n",
    "#と比較し、最初sに決定した差よりも大きい場合は上がり・下がりと\n",
    "#判断する\n",
    "#ただし、同時超えた場合はどちらが先に上がるか不明なためカウントしていない\n",
    "#もしかすると\n",
    "for i in range(len(low_predicted)):\n",
    "    high_temp = high_predicted[i] * diff_value + average_value\n",
    "    low_temp  = low_predicted[i] * diff_value + average_value\n",
    "    open_temp = data.loc[i+test_start_count+1, 'open'] * diff_value + average_value\n",
    "    close_temp = data.loc[i+test_start_count, 'close'] * diff_value + average_value\n",
    "\n",
    "    if high_temp - close_temp >= check_treshhold and \\\n",
    "        close_temp - low_temp < check_treshhold:\n",
    "        pre_check.append(1)\n",
    "        temp_check.append(high_temp - open_temp)\n",
    "        temp_close_open.append(abs(close_temp - open_temp))\n",
    "\n",
    "    elif close_temp- low_temp >= check_treshhold and \\\n",
    "        high_temp - close_temp < check_treshhold:\n",
    "        pre_check.append(-1)\n",
    "        temp_check.append(low_temp - open_temp)\n",
    "        temp_close_open.append(abs(close_temp - open_temp))\n",
    "    #elif high_temp - open_temp >= check_treshhold and \\\n",
    "    #    open_temp- low_temp >= check_treshhold:\n",
    "        \n",
    "    #    if high_temp - open_temp > open_temp- low_temp:\n",
    "    #        pre_check.append(1)\n",
    "    #        temp_check.append(high_temp - open_temp)\n",
    "    #    elif high_temp - open_temp < open_temp- low_temp:\n",
    "    #        pre_check.append(-1)\n",
    "    #        temp_check.append(low_temp - open_temp)\n",
    "    #    else:\n",
    "    #        pre_check.append(0)\n",
    "    #        temp_check.append(0)\n",
    "\n",
    "    #if high_temp - open_temp >= check_treshhold and \\\n",
    "    #    open_temp - low_temp < check_treshhold and \\\n",
    "    #    close_temp - open_temp <= up_c_o_diff and \\\n",
    "    #    close_temp - open_temp >= down_c_o_diff:\n",
    "    #    pre_check.append(1)\n",
    "    #    temp_check.append(high_temp - open_temp)\n",
    "\n",
    "    #elif open_temp- low_temp >= check_treshhold and \\\n",
    "    #    high_temp - open_temp < check_treshhold and \\\n",
    "    #    close_temp - open_temp <= up_c_o_diff and \\\n",
    "    #    close_temp - open_temp >= down_c_o_diff:\n",
    "    #    pre_check.append(-1)\n",
    "    #    temp_check.append(low_temp - open_temp)\n",
    "\n",
    "    else:\n",
    "        pre_check.append(0)\n",
    "        temp_check.append(0)\n",
    "\n",
    "up_ok_count = 0\n",
    "up_ng_count = 0\n",
    "up_ev_count = 0\n",
    "down_ok_count = 0\n",
    "down_ng_count = 0\n",
    "down_ev_count = 0\n",
    "high_win = numpy.array([])\n",
    "high_lost = numpy.array([])\n",
    "low_win  = numpy.array([])\n",
    "low_lost  = numpy.array([])\n",
    "\n",
    "for i in range(len(pre_check)):\n",
    "    if pre_check[i] == 1:\n",
    "        if up_down[i] == pre_check[i]:\n",
    "            up_ok_count += 1\n",
    "            high_win = numpy.append(high_win, numpy.array(temp_check[i]))\n",
    "        elif up_down[i] != pre_check[i] and up_down[i] == -1:\n",
    "            up_ng_count += 1\n",
    "            high_lost = numpy.append(high_lost, numpy.array(temp_check[i]))\n",
    "        else:\n",
    "            up_ev_count += 1\n",
    "            \n",
    "    elif pre_check[i] == -1:\n",
    "        if up_down[i] == pre_check[i]:\n",
    "            down_ok_count += 1\n",
    "            low_win = numpy.append(low_win, numpy.array(temp_check[i]))\n",
    "        elif up_down[i] != pre_check[i] and up_down[i] == 1:\n",
    "            down_ng_count += 1\n",
    "            low_lost = numpy.append(low_lost, numpy.array(temp_check[i]))\n",
    "        else:\n",
    "            down_ev_count += 1\n",
    "\n",
    "\n",
    "print('==========')\n",
    "print('UP:')\n",
    "print(' WIN  :' + str(up_ok_count))\n",
    "print(' LOST :' + str(up_ng_count))\n",
    "print(' DRAW :' + str(up_ev_count))\n",
    "print(' WIN RATE :' + str(math.floor((up_ok_count/(up_ok_count+up_ng_count))*100)) + '%')\n",
    "print('DOWN:')\n",
    "print(' WIN  :' + str(down_ok_count))\n",
    "print(' LOST :' + str(down_ng_count))\n",
    "print(' DRAW :' + str(down_ev_count))\n",
    "print(' WIN RATE :' + str(math.floor((down_ok_count/(down_ok_count+down_ng_count))*100)) + '%')\n",
    "#print('---------')\n",
    "#print('UP ')\n",
    "#print('  WIN  :' + str(high_win.mean()) + ' ' + str(high_win.std()))\n",
    "#print('  LOST  :' + str(high_lost.mean()) + ' ' + str(high_lost.std()))\n",
    "#print('DOWN ')\n",
    "#print('  WIN  :' + str(low_win.mean()) + ' ' + str(low_win.std()))\n",
    "#print('  LOST  :' + str(low_lost.mean()) + ' ' + str(low_lost.std()))\n",
    "\n",
    "\n",
    "print(str(math.floor(time.time() - starttime)) + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
